{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAUUSD AI Trading Model - Kaggle Training\n",
    "\n",
    "Train LSTM model on 55 years of XAUUSD historical data with multi-timeframe features.\n",
    "\n",
    "**Kaggle Advantages:**\n",
    "- Free GPU (P100 or T4)\n",
    "- 30GB RAM (vs Colab's 12GB)\n",
    "- 9-hour session limit\n",
    "- Persistent datasets\n",
    "\n",
    "**Before running:** Upload your `XAUUSD_HISTORICAL_DATA` folder as a Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check system resources\n",
    "!free -h\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas-ta joblib\n",
    "print(\"✓ Packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Dataset\n",
    "\n",
    "**IMPORTANT:** Before running this cell:\n",
    "1. Go to \"Add Data\" → \"Upload Dataset\"\n",
    "2. Upload your `XAUUSD_HISTORICAL_DATA` folder\n",
    "3. Name it: `xauusd-historical-data`\n",
    "4. Make it public or private\n",
    "5. Then run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available datasets\n",
    "!ls /kaggle/input/\n",
    "\n",
    "# Set data path (update if your dataset name is different)\n",
    "DATA_DIR = '/kaggle/input/xauusd-historical-data'\n",
    "\n",
    "# Verify data exists\n",
    "!ls -lh \"$DATA_DIR\"/*.csv | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = '/kaggle/working/processed_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_mt_csv(filepath):\n",
    "    \"\"\"Load MetaTrader CSV format\"\"\"\n",
    "    print(f\"\\nLoading: {os.path.basename(filepath)}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            sep='\\t',\n",
    "            skiprows=0,\n",
    "            names=['DATE', 'TIME', 'OPEN', 'HIGH', 'LOW', 'CLOSE', 'TICKVOL', 'VOL', 'SPREAD'],\n",
    "            skipinitialspace=True\n",
    "        )\n",
    "        \n",
    "        if df['DATE'].iloc[0] == '<DATE>':\n",
    "            df = df.iloc[1:]\n",
    "        \n",
    "        df['DateTime'] = pd.to_datetime(\n",
    "            df['DATE'].astype(str) + ' ' + df['TIME'].astype(str),\n",
    "            format='%Y.%m.%d %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        df.set_index('DateTime', inplace=True)\n",
    "        df.rename(columns={\n",
    "            'OPEN': 'Open',\n",
    "            'HIGH': 'High',\n",
    "            'LOW': 'Low',\n",
    "            'CLOSE': 'Close',\n",
    "            'TICKVOL': 'Volume'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        \n",
    "        for col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "        print(f\"  ✓ Loaded {len(df):,} bars\")\n",
    "        print(f\"  ✓ Range: {df.index[0]} to {df.index[-1]}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Data processing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority timeframes\n",
    "timeframes = {\n",
    "    'M5': 'XAUUSD_M5_*.csv',\n",
    "    'M15': 'XAUUSD_M15_*.csv',\n",
    "    'H1': 'XAUUSD_H1_*.csv',\n",
    "    'H4': 'XAUUSD_H4_*.csv'\n",
    "}\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "for tf_name, pattern in timeframes.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {tf_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    matching_files = glob.glob(os.path.join(DATA_DIR, pattern))\n",
    "    \n",
    "    if matching_files:\n",
    "        df = load_mt_csv(matching_files[0])\n",
    "        \n",
    "        if df is not None:\n",
    "            # Save processed data\n",
    "            output_file = os.path.join(OUTPUT_DIR, f\"XAUUSD_{tf_name}_processed.csv\")\n",
    "            df.to_csv(output_file)\n",
    "            processed_data[tf_name] = df\n",
    "            print(f\"  ✓ Saved: {output_file}\")\n",
    "    else:\n",
    "        print(f\"  ✗ No file found for {tf_name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ Data Processing Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for tf, df in processed_data.items():\n",
    "    print(f\"{tf}: {len(df):,} bars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import mixed_precision\n",
    "import joblib\n",
    "\n",
    "# Enable mixed precision for faster training on GPU\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(\"✓ Mixed precision enabled\")\n",
    "\n",
    "# Configuration\n",
    "SEQ_LEN = 120\n",
    "FUTURE_TARGET = 1\n",
    "PROCESSED_DATA_DIR = '/kaggle/working/processed_data'\n",
    "\n",
    "def load_data(csv_path):\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Loading: {os.path.basename(csv_path)}...\")\n",
    "        df = pd.read_csv(csv_path, parse_dates=['DateTime'], index_col='DateTime')\n",
    "        print(f\"  ✓ {len(df):,} rows\")\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def add_smc_indicators(df):\n",
    "    df['Swing_High'] = df['High'].rolling(window=5, center=True).max() == df['High']\n",
    "    df['Swing_Low'] = df['Low'].rolling(window=5, center=True).min() == df['Low']\n",
    "    df['Last_Swing_High'] = df['High'].where(df['Swing_High']).ffill()\n",
    "    df['Last_Swing_Low'] = df['Low'].where(df['Swing_Low']).ffill()\n",
    "    df['Dist_to_High'] = df['Last_Swing_High'] - df['Close']\n",
    "    df['Dist_to_Low'] = df['Close'] - df['Last_Swing_Low']\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_technical_indicators(df, prefix=''):\n",
    "    df[f'{prefix}EMA_50'] = ta.ema(df['Close'], length=50)\n",
    "    df[f'{prefix}EMA_200'] = ta.ema(df['Close'], length=200)\n",
    "    df[f'{prefix}RSI'] = ta.rsi(df['Close'], length=14)\n",
    "    \n",
    "    macd = ta.macd(df['Close'])\n",
    "    if isinstance(macd, pd.DataFrame):\n",
    "        df[f'{prefix}MACD'] = macd.iloc[:, 0]\n",
    "    else:\n",
    "        df[f'{prefix}MACD'] = macd\n",
    "    \n",
    "    df[f'{prefix}ATR'] = ta.atr(df['High'], df['Low'], df['Close'], length=14)\n",
    "    \n",
    "    bb = ta.bbands(df['Close'], length=20)\n",
    "    if bb is not None:\n",
    "        df[f'{prefix}BB_UPPER'] = bb.iloc[:, 0]\n",
    "        df[f'{prefix}BB_LOWER'] = bb.iloc[:, 2]\n",
    "    \n",
    "    if prefix == '':\n",
    "        df = add_smc_indicators(df)\n",
    "    \n",
    "    if 'Volume' in df.columns:\n",
    "        df[f'{prefix}Volume_MA'] = df['Volume'].rolling(window=20).mean()\n",
    "        df[f'{prefix}Volume_Ratio'] = df['Volume'] / df[f'{prefix}Volume_MA']\n",
    "    \n",
    "    df[f'{prefix}Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def align_higher_timeframe(df_main, df_htf, prefix):\n",
    "    print(f\"  Aligning {prefix}...\")\n",
    "    htf_cols = [col for col in df_htf.columns if col not in ['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    df_htf_selected = df_htf[htf_cols].copy()\n",
    "    df_htf_selected.columns = [f'{prefix}{col}' for col in df_htf_selected.columns]\n",
    "    df_merged = df_main.join(df_htf_selected, how='left')\n",
    "    df_merged.fillna(method='ffill', inplace=True)\n",
    "    return df_merged\n",
    "\n",
    "def preprocess_data(df, use_sampling=True):\n",
    "    if use_sampling:\n",
    "        print(f\"\\nApplying sampling...\")\n",
    "        print(f\"  Original: {len(df):,} rows\")\n",
    "        \n",
    "        cutoff_date = df.index[-1] - pd.Timedelta(days=730)\n",
    "        df_recent = df[df.index >= cutoff_date]\n",
    "        df_old = df[df.index < cutoff_date]\n",
    "        \n",
    "        if len(df_old) > 0:\n",
    "            sample_size = int(len(df_old) * 0.2)\n",
    "            df_old_sampled = df_old.sample(n=sample_size, random_state=42).sort_index()\n",
    "            df = pd.concat([df_old_sampled, df_recent])\n",
    "        else:\n",
    "            df = df_recent\n",
    "        \n",
    "        print(f\"  Sampled: {len(df):,} rows\")\n",
    "    \n",
    "    df['Target'] = df['Log_Return'].shift(-FUTURE_TARGET)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    feature_cols = [\n",
    "        'Close', 'RSI', 'MACD', 'ATR', 'EMA_50', 'EMA_200', 'BB_UPPER', 'BB_LOWER',\n",
    "        'Dist_to_High', 'Dist_to_Low', 'Volume_MA', 'Volume_Ratio'\n",
    "    ]\n",
    "    \n",
    "    htf_features = [col for col in df.columns if col.startswith('HTF')]\n",
    "    feature_cols.extend(htf_features)\n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"\\nUsing {len(feature_cols)} features\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "    target_scaler = MinMaxScaler()\n",
    "    scaled_target = target_scaler.fit_transform(df[['Target']])\n",
    "    \n",
    "    X, y = [], []\n",
    "    print(f\"Creating sequences...\")\n",
    "    for i in range(SEQ_LEN, len(scaled_data)):\n",
    "        X.append(scaled_data[i-SEQ_LEN:i])\n",
    "        y.append(scaled_target[i])\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"  {i:,} / {len(scaled_data):,}\")\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler, target_scaler, feature_cols\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=256, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=256, return_sequences=True, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=128, return_sequences=False, kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "print(\"✓ Training functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING XAUUSD MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "print(\"\\n--- Loading Data ---\")\n",
    "df_5m = load_data(os.path.join(PROCESSED_DATA_DIR, 'XAUUSD_M5_processed.csv'))\n",
    "df_15m = load_data(os.path.join(PROCESSED_DATA_DIR, 'XAUUSD_M15_processed.csv'))\n",
    "df_1h = load_data(os.path.join(PROCESSED_DATA_DIR, 'XAUUSD_H1_processed.csv'))\n",
    "df_4h = load_data(os.path.join(PROCESSED_DATA_DIR, 'XAUUSD_H4_processed.csv'))\n",
    "\n",
    "# Add indicators\n",
    "print(\"\\n--- Adding Indicators ---\")\n",
    "df_5m = add_technical_indicators(df_5m, prefix='')\n",
    "\n",
    "if df_15m is not None:\n",
    "    df_15m = add_technical_indicators(df_15m, prefix='')\n",
    "    df_5m = align_higher_timeframe(df_5m, df_15m, 'HTF15_')\n",
    "\n",
    "if df_1h is not None:\n",
    "    df_1h = add_technical_indicators(df_1h, prefix='')\n",
    "    df_5m = align_higher_timeframe(df_5m, df_1h, 'HTF1H_')\n",
    "\n",
    "if df_4h is not None:\n",
    "    df_4h = add_technical_indicators(df_4h, prefix='')\n",
    "    df_5m = align_higher_timeframe(df_5m, df_4h, 'HTF4H_')\n",
    "\n",
    "# Preprocess\n",
    "print(\"\\n--- Preprocessing ---\")\n",
    "X, y, scaler, target_scaler, features = preprocess_data(df_5m, use_sampling=True)\n",
    "\n",
    "# Split\n",
    "split = int(len(X) * 0.9)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"\\n--- Data Split ---\")\n",
    "print(f\"Training: {X_train.shape[0]:,} sequences\")\n",
    "print(f\"Testing: {X_test.shape[0]:,} sequences\")\n",
    "print(f\"Features: {X_train.shape[2]}\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "print(\"\\n✓ Scalers saved\")\n",
    "\n",
    "# Build model\n",
    "print(\"\\n--- Building Model ---\")\n",
    "model = build_model((X_train.shape[1], X_train.shape[2]))\n",
    "print(f\"Parameters: {model.count_params():,}\")\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"best_xauusd_model.keras\", save_best_only=True, \n",
    "                            monitor='val_loss', mode='min', verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
    "                              min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Train\n",
    "print(\"\\n--- Training ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[checkpoint, early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ TRAINING COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model('best_xauusd_model.keras')\n",
    "target_scaler = joblib.load('target_scaler.pkl')\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "predictions_actual = target_scaler.inverse_transform(predictions)\n",
    "y_test_actual = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test_actual, predictions_actual)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nMAE on Log Returns: {mae:.6f}\")\n",
    "print(f\"Approximate Price Error: ${mae * 2915:.2f}\")\n",
    "\n",
    "# Directional accuracy\n",
    "correct = sum((y_test_actual[i][0] > 0 and predictions_actual[i][0] > 0) or \n",
    "              (y_test_actual[i][0] < 0 and predictions_actual[i][0] < 0) \n",
    "              for i in range(len(y_test_actual)))\n",
    "accuracy = (correct / len(y_test_actual)) * 100\n",
    "print(f\"\\nDirectional Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files exist\n",
    "!ls -lh best_xauusd_model.keras scaler.pkl target_scaler.pkl\n",
    "\n",
    "print(\"\\n✓ Model files ready!\")\n",
    "print(\"\\nTo download:\")\n",
    "print(\"1. Click 'Output' tab on the right\")\n",
    "print(\"2. Download all 3 files\")\n",
    "print(\"3. Copy to your local project folder\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
